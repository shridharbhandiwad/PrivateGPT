version: '3.8'

services:
  zoppler-radar-ai:
    build: .
    container_name: zoppler-radar-ai
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST:-http://host.docker.internal:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2}
      - HOST=0.0.0.0
      - PORT=8000
    env_file:
      - .env
    restart: unless-stopped
    # On Linux, use host networking to access Ollama on host
    # Uncomment the next line and comment out 'ports' on Linux:
    # network_mode: "host"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      # Mount for development (optional)
      - ./app.py:/app/app.py
      - ./static:/app/static
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

# Alternative: Run Ollama in Docker (requires GPU for good performance)
# Uncomment to include Ollama service:
#
#  ollama:
#    image: ollama/ollama:latest
#    container_name: ollama
#    ports:
#      - "11434:11434"
#    volumes:
#      - ollama_data:/root/.ollama
#    restart: unless-stopped
#    # For GPU support (NVIDIA), uncomment:
#    # deploy:
#    #   resources:
#    #     reservations:
#    #       devices:
#    #         - driver: nvidia
#    #           count: 1
#    #           capabilities: [gpu]
#
#volumes:
#  ollama_data:
