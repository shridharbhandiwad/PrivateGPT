# Zoppler Radar AI Configuration - Local LLM
# Copy this file to .env and customize as needed

# Ollama Configuration
# Ollama host URL (default: http://localhost:11434)
OLLAMA_HOST=http://localhost:11434

# Ollama model to use (default: llama3.2)
# Popular options: llama3.2, llama3.1, llama2, mistral, codellama, phi3
OLLAMA_MODEL=llama3.2

# Server Configuration (optional)
HOST=0.0.0.0
PORT=8000

# Environment
ENVIRONMENT=production
